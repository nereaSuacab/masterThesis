{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24481ef7",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082641f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nerea\\anaconda3\\envs\\thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import NonLLMContextPrecisionWithReference\n",
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addb58c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nerea\\AppData\\Local\\Temp\\ipykernel_24236\\1909013780.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.1\")\n",
      "C:\\Users\\nerea\\AppData\\Local\\Temp\\ipykernel_24236\\1909013780.py:5: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluator_llm = LangchainLLMWrapper(llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "evaluator_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a54ae",
   "metadata": {},
   "source": [
    "Read Ground truth and get var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199e1224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Ground truth answer: Potential products are Sound Level Meter 2245, Sound Level Meter 2255, Building Acoustic Software and Accessories and DIRAC software.\n",
      "Number of ground truth texts: 17\n",
      "First text snippet: You can simultaneously measure source (L1) and receiving room levels (L2) by connecting two HBK 2255 Sound Level Meters to the app.  This feature is designed to save time and enhance efficiency, espec ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = r\"gt.json\"\n",
    "\n",
    "# Load the JSON safely\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build a list of tuples: (query, ground_truth_answer, [list of reference texts])\n",
    "query_texts_pairs = [\n",
    "    (\n",
    "        item[\"question\"].strip(),\n",
    "        item[\"ground_truth_answer\"].strip(),\n",
    "        [seg[\"text\"].strip() for seg in item.get(\"ground_truth_segments\", [])]\n",
    "    )\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Example: show the first query and its data\n",
    "first_query, first_gt_answer, first_texts = query_texts_pairs[0]\n",
    "print(\"Query:\", first_query)\n",
    "print(\"Ground truth answer:\", first_gt_answer)\n",
    "print(\"Number of ground truth texts:\", len(first_texts))\n",
    "print(\"First text snippet:\", first_texts[0][:200].replace(\"\\n\", \" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ad22d",
   "metadata": {},
   "source": [
    "read dense json and get question and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea936817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Number of retrieved texts: 2\n",
      "First retrieved text snippet: Pros and cons of measurement methods Scans can be faster than measuring at fixed pos- You are able to listen to the sound field as you You can control measurements from outside the room, without intro ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your RAG output JSON\n",
    "path = r\"retrieval_results.json\"\n",
    "\n",
    "# Load the JSON\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    rag_data_dense = json.load(f)\n",
    "\n",
    "# Build a list of tuples: (query, [list of retrieved texts])\n",
    "query_retrieved_pairs_dense = [\n",
    "    (\n",
    "        query,\n",
    "        [item[\"text\"].strip() for item in texts]\n",
    "    )\n",
    "    for query, texts in rag_data_dense.items()\n",
    "]\n",
    "\n",
    "# Example: show the first query and its retrieved texts\n",
    "first_query, first_texts = query_retrieved_pairs_dense[0]\n",
    "print(\"Query:\", first_query)\n",
    "print(\"Number of retrieved texts:\", len(first_texts))\n",
    "print(\"First retrieved text snippet:\", first_texts[0][:200].replace(\"\\n\", \" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c853d9",
   "metadata": {},
   "source": [
    "Same with the sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e33200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Number of retrieved texts: 2\n",
      "First retrieved text snippet: It is possible to override all data.  Adjust  the slope of the decay, if needed, or edit data to test theories  about the effects of changes you can make to get specific results.   Data edited in this ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your BM25 output JSON\n",
    "path = r\"retrieval_results_sparse copy.json\"\n",
    "\n",
    "# Load the JSON\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bm25_data = json.load(f)\n",
    "\n",
    "# Build a list of tuples: (query, [list of retrieved texts])\n",
    "query_retrieved_pairs_sparse = [\n",
    "    (\n",
    "        item[\"query\"],\n",
    "        [res[\"window\"].strip() for res in item[\"results\"]]\n",
    "    )\n",
    "    for item in bm25_data\n",
    "]\n",
    "\n",
    "# Example: show the first query and its retrieved texts\n",
    "first_query, first_texts = query_retrieved_pairs_sparse[0]\n",
    "print(\"Query:\", first_query)\n",
    "print(\"Number of retrieved texts:\", len(first_texts))\n",
    "print(\"First retrieved text snippet:\", first_texts[0][:200].replace(\"\\n\", \" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9903e9a",
   "metadata": {},
   "source": [
    "LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa255af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense pairs found: 18\n",
      "Sparse pairs found: 18\n"
     ]
    }
   ],
   "source": [
    "# Load chat logs and separate by mode\n",
    "path = r\"chat_logs.json\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dense_pairs = []\n",
    "sparse_pairs = []\n",
    "\n",
    "for item in data:\n",
    "    query = item.get(\"query\", \"\").strip()\n",
    "    mode = item.get(\"mode\", \"\").strip().lower()\n",
    "    response = item.get(\"response\", \"\").strip()\n",
    "    \n",
    "    if mode == \"dense rag\":\n",
    "        dense_pairs.append((query, response))\n",
    "    elif mode == \"sparse rag\":\n",
    "        sparse_pairs.append((query, response))\n",
    "\n",
    "print(\"Dense pairs found:\", len(dense_pairs))\n",
    "print(\"Sparse pairs found:\", len(sparse_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceade81b",
   "metadata": {},
   "source": [
    "### Context Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa156f67",
   "metadata": {},
   "source": [
    "#### DENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c4a94",
   "metadata": {},
   "source": [
    "Context Precision WITH reference - NON LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d8670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense - Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution should I use if I want compatibility with analysis tools like matlab?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution is better for a university group? They want to use this solution in different applications.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need a solution that complies with noise regulations and does automated reports.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution should I use to measure noise levels in a factory floor?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: What solution should I use to see if a construction site follows the noise regulations?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution should I use to conduct noise impact assessments? I need GPS tagging and the device to be weatherproof.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I want to monitore a contruction site. What should I use? I need long term logging and report generation.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need a solution that covers, Long term logging, GPS tagging, weatherproofing and report generation.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution should I use to measure a building design?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need a solution for covering Frequency analysis, reverberation time and standards compliance.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need to measure a residential building and I need to follow the standard ISO 3382. Which solution should I use?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which is the best solution to measure room acoustics?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need to perform sound power measurements with a multi channel setup. Which solution should I use?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need a solution that covers standard compliance to perform sound power measurements.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I want a solution with a simple interface and affordabe.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I would like to have a solution for noise monitoring that is portable.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solutions gives basic logging and a simple interface?\n",
      "Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure prerequisite variables are available\n",
    "if 'query_texts_pairs' not in globals() or 'query_retrieved_pairs_dense' not in globals():\n",
    "    raise NameError(\n",
    "        \"query_texts_pairs and/or query_retrieved_pairs_dense are not defined. \"\n",
    "        \"Please run the cells that load ground truth (cell that creates query_texts_pairs) \"\n",
    "        \"and dense retrieval results (cell that creates query_retrieved_pairs_dense) before this cell.\"\n",
    "    )\n",
    "\n",
    "# Initialize metric\n",
    "context_precision = NonLLMContextPrecisionWithReference()\n",
    "\n",
    "# Store scores\n",
    "dense_scores = []\n",
    "sparse_scores = []\n",
    "\n",
    "# Loop over all queries for Dense retrieval\n",
    "# Note: query_texts_pairs elements are (query, ground_truth_answer, [reference_texts])\n",
    "for (query_gt, gt_answer, gt_texts), (query_dense, dense_texts) in zip(query_texts_pairs, query_retrieved_pairs_dense):\n",
    "    # Sanity check: queries should match\n",
    "    assert query_gt == query_dense, f\"Query mismatch: {query_gt} vs {query_dense}\"\n",
    "\n",
    "    # Build sample (NonLLM metric expects reference_contexts + retrieved_contexts)\n",
    "    sample = SingleTurnSample(\n",
    "        retrieved_contexts=dense_texts,\n",
    "        reference_contexts=gt_texts\n",
    "    )\n",
    "\n",
    "    # Compute score (async)\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    dense_scores.append((query_gt, score))\n",
    "    print(f\"Dense - Query: {query_gt}\\nScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138121a",
   "metadata": {},
   "source": [
    "Context Precision WITH Reference - LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d7af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Dense RAG (With Reference) ===\n",
      "Dense - Query: Which solution should a researcher use if they want High acc... Score: 0.0\n",
      "Dense - Query: Which solution should I use if I want compatibility with ana... Score: 0.0\n",
      "Dense - Query: Which solution is better for a university group? They want t... Score: 0.0\n",
      "Dense - Query: I need a solution that complies with noise regulations and d... Score: 0.99999999995\n",
      "Dense - Query: Which solution should I use to measure noise levels in a fac... Score: 0.0\n",
      "Dense - Query: What solution should I use to see if a construction site fol... Score: 0.99999999995\n",
      "Dense - Query: Which solution should I use to conduct noise impact assessme... Score: 0.0\n",
      "Dense - Query: I want to monitore a contruction site. What should I use? I ... Score: 0.0\n",
      "Dense - Query: I need a solution that covers, Long term logging, GPS taggin... Score: 0.0\n",
      "Dense - Query: Which solution should I use to measure a building design?... Score: 0.0\n",
      "Dense - Query: I need a solution for covering Frequency analysis, reverbera... Score: 0.0\n",
      "Dense - Query: I need to measure a residential building and I need to follo... Score: 0.0\n",
      "Dense - Query: Which is the best solution to measure room acoustics?... Score: 0.0\n",
      "Dense - Query: I need to perform sound power measurements with a multi chan... Score: 0.0\n",
      "Dense - Query: I need a solution that covers standard compliance to perform... Score: 0.99999999995\n",
      "Dense - Query: I want a solution with a simple interface and affordabe.... Score: 0.0\n",
      "Dense - Query: I would like to have a solution for noise monitoring that is... Score: 0.0\n",
      "Dense - Query: Which solutions gives basic logging and a simple interface?... Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
    "\n",
    "# Evaluate Dense RAG\n",
    "print(\"\\n=== Evaluating Dense RAG (With Reference) ===\")\n",
    "dense_scores = []\n",
    "\n",
    "for (query_gt, gt_answer, gt_texts), (query_dense, dense_texts) in zip(\n",
    "    query_texts_pairs, \n",
    "    query_retrieved_pairs_dense\n",
    "):\n",
    "    assert query_gt == query_dense, f\"Query mismatch\"\n",
    "\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        reference=gt_answer,  # Ground truth answer\n",
    "        retrieved_contexts=dense_texts\n",
    "    )\n",
    "\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    print(f\"Dense - Query: {query_gt[:60]}... Score: {score}\")\n",
    "    dense_scores.append((query_gt, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c4064c",
   "metadata": {},
   "source": [
    "Context Precision Without Reference - LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c89d0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Dense RAG ===\n",
      "Dense - Query: Which solution should a researcher use if they want High acc... Score: 0.0\n",
      "Dense - Query: Which solution should I use if I want compatibility with ana... Score: 0.9999999999\n",
      "Dense - Query: Which solution is better for a university group? They want t... Score: 0.0\n",
      "Dense - Query: I need a solution that complies with noise regulations and d... Score: 0.99999999995\n",
      "Dense - Query: Which solution should I use to measure noise levels in a fac... Score: 0.99999999995\n",
      "Dense - Query: What solution should I use to see if a construction site fol... Score: 0.99999999995\n",
      "Dense - Query: Which solution should I use to conduct noise impact assessme... Score: 0.0\n",
      "Dense - Query: I want to monitore a contruction site. What should I use? I ... Score: 0.49999999995\n",
      "Dense - Query: I need a solution that covers, Long term logging, GPS taggin... Score: 0.0\n",
      "Dense - Query: Which solution should I use to measure a building design?... Score: 0.0\n",
      "Dense - Query: I need a solution for covering Frequency analysis, reverbera... Score: 0.99999999995\n",
      "Dense - Query: I need to measure a residential building and I need to follo... Score: 0.99999999995\n",
      "Dense - Query: Which is the best solution to measure room acoustics?... Score: 0.99999999995\n",
      "Dense - Query: I need to perform sound power measurements with a multi chan... Score: 0.99999999995\n",
      "Dense - Query: I need a solution that covers standard compliance to perform... Score: 0.99999999995\n",
      "Dense - Query: I want a solution with a simple interface and affordabe.... Score: 0.0\n",
      "Dense - Query: I would like to have a solution for noise monitoring that is... Score: 0.99999999995\n",
      "Dense - Query: Which solutions gives basic logging and a simple interface?... Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "context_precision = LLMContextPrecisionWithoutReference(llm=evaluator_llm)\n",
    "\n",
    "# Evaluate Dense RAG\n",
    "print(\"\\n=== Evaluating Dense RAG ===\")\n",
    "dense_scores = []\n",
    "\n",
    "for (query_gt, gt_answer, gt_texts), (query_dense, dense_texts), (query_response, response) in zip(\n",
    "    query_texts_pairs, \n",
    "    query_retrieved_pairs_dense, \n",
    "    dense_pairs\n",
    "):\n",
    "    # Sanity check\n",
    "    assert query_gt == query_dense == query_response, f\"Query mismatch: {query_gt} vs {query_dense} vs {query_response}\"\n",
    "\n",
    "    # Build sample\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        response=response,  # LLM's actual response\n",
    "        retrieved_contexts=dense_texts\n",
    "    )\n",
    "\n",
    "    # Compute score\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    print(f\"Dense - Query: {query_gt[:60]}... Score: {score}\")\n",
    "    dense_scores.append((query_gt, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbaf77",
   "metadata": {},
   "source": [
    "#### Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d887fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse - Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution should I use if I want compatibility with analysis tools like matlab?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution is better for a university group? They want to use this solution in different applications.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need a solution that complies with noise regulations and does automated reports.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution should I use to measure noise levels in a factory floor?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: What solution should I use to see if a construction site follows the noise regulations?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution should I use to conduct noise impact assessments? I need GPS tagging and the device to be weatherproof.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I want to monitore a contruction site. What should I use? I need long term logging and report generation.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need a solution that covers, Long term logging, GPS tagging, weatherproofing and report generation.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution should I use to measure a building design?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need a solution for covering Frequency analysis, reverberation time and standards compliance.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need to measure a residential building and I need to follow the standard ISO 3382. Which solution should I use?\n",
      "Score: 0.99999999995\n",
      "\n",
      "Sparse - Query: Which is the best solution to measure room acoustics?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need to perform sound power measurements with a multi channel setup. Which solution should I use?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need a solution that covers standard compliance to perform sound power measurements.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I want a solution with a simple interface and affordabe.\n",
      "Score: 0.9999999999\n",
      "\n",
      "Sparse - Query: I would like to have a solution for noise monitoring that is portable.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solutions gives basic logging and a simple interface?\n",
      "Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure prerequisite variables are available\n",
    "if 'query_texts_pairs' not in globals() or 'query_retrieved_pairs_dense' not in globals():\n",
    "    raise NameError(\n",
    "        \"query_texts_pairs and/or query_retrieved_pairs_dense are not defined. \"\n",
    "        \"Please run the cells that load ground truth (cell that creates query_texts_pairs) \"\n",
    "        \"and dense retrieval results (cell that creates query_retrieved_pairs_dense) before this cell.\"\n",
    "    )\n",
    "\n",
    "# Initialize metric\n",
    "context_precision = NonLLMContextPrecisionWithReference()\n",
    "\n",
    "\n",
    "sparse_scores = []\n",
    "\n",
    "# Loop over all queries for Sparse retrieval\n",
    "# Note: query_texts_pairs elements are (query, ground_truth_answer, [reference_texts])\n",
    "for (query_gt, gt_answer, gt_texts), (query_sparse, sparse_texts) in zip(query_texts_pairs, query_retrieved_pairs_sparse):\n",
    "    # Sanity check: queries should match\n",
    "    assert query_gt == query_sparse, f\"Query mismatch: {query_gt} vs {query_sparse}\"\n",
    "\n",
    "    # Build sample (NonLLM metric expects reference_contexts + retrieved_contexts)\n",
    "    sample = SingleTurnSample(\n",
    "        retrieved_contexts=sparse_texts,\n",
    "        reference_contexts=gt_texts\n",
    "    )\n",
    "\n",
    "    # Compute score (async)\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    sparse_scores.append((query_gt, score))\n",
    "    print(f\"Sparse - Query: {query_gt}\\nScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce148fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Sparse RAG (With Reference) ===\n",
      "Sparse - Query: Which solution should a researcher use if they want High acc... Score: 0.0\n",
      "Sparse - Query: Which solution should I use if I want compatibility with ana... Score: 0.0\n",
      "Sparse - Query: Which solution is better for a university group? They want t... Score: 0.0\n",
      "Sparse - Query: I need a solution that complies with noise regulations and d... Score: 0.9999999999\n",
      "Sparse - Query: Which solution should I use to measure noise levels in a fac... Score: 0.0\n",
      "Sparse - Query: What solution should I use to see if a construction site fol... Score: 0.49999999995\n",
      "Sparse - Query: Which solution should I use to conduct noise impact assessme... Score: 0.9999999999\n",
      "Sparse - Query: I want to monitore a contruction site. What should I use? I ... Score: 0.0\n",
      "Sparse - Query: I need a solution that covers, Long term logging, GPS taggin... Score: 0.0\n",
      "Sparse - Query: Which solution should I use to measure a building design?... Score: 0.0\n",
      "Sparse - Query: I need a solution for covering Frequency analysis, reverbera... Score: 0.0\n",
      "Sparse - Query: I need to measure a residential building and I need to follo... Score: 0.0\n",
      "Sparse - Query: Which is the best solution to measure room acoustics?... Score: 0.0\n",
      "Sparse - Query: I need to perform sound power measurements with a multi chan... Score: 0.0\n",
      "Sparse - Query: I need a solution that covers standard compliance to perform... Score: 0.99999999995\n",
      "Sparse - Query: I want a solution with a simple interface and affordabe.... Score: 0.99999999995\n",
      "Sparse - Query: I would like to have a solution for noise monitoring that is... Score: 0.0\n",
      "Sparse - Query: Which solutions gives basic logging and a simple interface?... Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
    "\n",
    "# Evaluate Sparse RAG\n",
    "print(\"\\n=== Evaluating Sparse RAG (With Reference) ===\")\n",
    "sparse_scores = []\n",
    "\n",
    "for (query_gt, gt_answer, gt_texts), (query_sparse, sparse_texts) in zip(\n",
    "    query_texts_pairs, \n",
    "    query_retrieved_pairs_sparse\n",
    "):\n",
    "    assert query_gt == query_sparse, f\"Query mismatch: {query_gt} vs {query_sparse}\"\n",
    "\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        reference=gt_answer,  # Ground truth answer\n",
    "        retrieved_contexts=sparse_texts\n",
    "    )\n",
    "\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    print(f\"Sparse - Query: {query_gt[:60]}... Score: {score}\")\n",
    "    sparse_scores.append((query_gt, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a218e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Sparse RAG ===\n",
      "Sparse - Query: Which solution should a researcher use if they want High acc... Score: 0.9999999999\n",
      "Sparse - Query: Which solution should I use if I want compatibility with ana... Score: 0.0\n",
      "Sparse - Query: Which solution is better for a university group? They want t... Score: 0.99999999995\n",
      "Sparse - Query: I need a solution that complies with noise regulations and d... Score: 0.99999999995\n",
      "Sparse - Query: Which solution should I use to measure noise levels in a fac... Score: 0.0\n",
      "Sparse - Query: What solution should I use to see if a construction site fol... Score: 0.49999999995\n",
      "Sparse - Query: Which solution should I use to conduct noise impact assessme... Score: 0.9999999999\n",
      "Sparse - Query: I want to monitore a contruction site. What should I use? I ... Score: 0.9999999999\n",
      "Sparse - Query: I need a solution that covers, Long term logging, GPS taggin... Score: 0.0\n",
      "Sparse - Query: Which solution should I use to measure a building design?... Score: 0.9999999999\n",
      "Sparse - Query: I need a solution for covering Frequency analysis, reverbera... Score: 0.99999999995\n",
      "Sparse - Query: I need to measure a residential building and I need to follo... Score: 0.49999999995\n",
      "Sparse - Query: Which is the best solution to measure room acoustics?... Score: 0.0\n",
      "Sparse - Query: I need to perform sound power measurements with a multi chan... Score: 0.99999999995\n",
      "Sparse - Query: I need a solution that covers standard compliance to perform... Score: 0.99999999995\n",
      "Sparse - Query: I want a solution with a simple interface and affordabe.... Score: 0.99999999995\n",
      "Sparse - Query: I would like to have a solution for noise monitoring that is... Score: 0.49999999995\n",
      "Sparse - Query: Which solutions gives basic logging and a simple interface?... Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "context_precision = LLMContextPrecisionWithoutReference(llm=evaluator_llm)\n",
    "\n",
    "# Evaluate Sparse RAG\n",
    "print(\"\\n=== Evaluating Sparse RAG ===\")\n",
    "sparse_scores = []\n",
    "\n",
    "for (query_gt, gt_answer, gt_texts), (query_sparse, sparse_texts), (query_response, response) in zip(\n",
    "    query_texts_pairs, \n",
    "    query_retrieved_pairs_sparse, \n",
    "    sparse_pairs\n",
    "):\n",
    "    # Sanity check\n",
    "    assert query_gt == query_sparse == query_response, f\"Query mismatch: {query_gt} vs {query_sparse} vs {query_response}\"\n",
    "\n",
    "    # Build sample\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        response=response,  # LLM's actual response\n",
    "        retrieved_contexts=sparse_texts\n",
    "    )\n",
    "\n",
    "    # Compute score\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    print(f\"Sparse - Query: {query_gt[:60]}... Score: {score}\")\n",
    "    sparse_scores.append((query_gt, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
