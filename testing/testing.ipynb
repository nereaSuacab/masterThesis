{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24481ef7",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082641f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nerea\\anaconda3\\envs\\thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import NonLLMContextPrecisionWithReference\n",
    "from ragas.metrics import LLMContextPrecisionWithReference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addb58c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nerea\\AppData\\Local\\Temp\\ipykernel_10140\\1909013780.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.1\")\n",
      "C:\\Users\\nerea\\AppData\\Local\\Temp\\ipykernel_10140\\1909013780.py:5: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluator_llm = LangchainLLMWrapper(llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "evaluator_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a54ae",
   "metadata": {},
   "source": [
    "Read Ground truth and get var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199e1224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Ground truth answer: Potential products are Sound Level Meter 2245, Sound Level Meter 2255, Building Acoustic Software and Accessories and DIRAC software.\n",
      "Number of ground truth texts: 17\n",
      "First text snippet: You can simultaneously measure source (L1) and receiving room levels (L2) by connecting two HBK 2255 Sound Level Meters to the app.  This feature is designed to save time and enhance efficiency, espec ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = r\"gt.json\"\n",
    "\n",
    "# Load the JSON safely\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build a list of tuples: (query, ground_truth_answer, [list of reference texts])\n",
    "query_texts_pairs = [\n",
    "    (\n",
    "        item[\"question\"].strip(),\n",
    "        item[\"ground_truth_answer\"].strip(),\n",
    "        [seg[\"text\"].strip() for seg in item.get(\"ground_truth_segments\", [])]\n",
    "    )\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Example: show the first query and its data\n",
    "first_query, first_gt_answer, first_texts = query_texts_pairs[0]\n",
    "print(\"Query:\", first_query)\n",
    "print(\"Ground truth answer:\", first_gt_answer)\n",
    "print(\"Number of ground truth texts:\", len(first_texts))\n",
    "print(\"First text snippet:\", first_texts[0][:200].replace(\"\\n\", \" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ad22d",
   "metadata": {},
   "source": [
    "read dense json and get question and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea936817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Number of retrieved texts: 2\n",
      "First retrieved text snippet: Pros and cons of measurement methods Scans can be faster than measuring at fixed pos- You are able to listen to the sound field as you You can control measurements from outside the room, without intro ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your RAG output JSON\n",
    "path = r\"retrieval_results.json\"\n",
    "\n",
    "# Load the JSON\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    rag_data_dense = json.load(f)\n",
    "\n",
    "# Build a list of tuples: (query, [list of retrieved texts])\n",
    "query_retrieved_pairs_dense = [\n",
    "    (\n",
    "        query,\n",
    "        [item[\"text\"].strip() for item in texts]\n",
    "    )\n",
    "    for query, texts in rag_data_dense.items()\n",
    "]\n",
    "\n",
    "# Example: show the first query and its retrieved texts\n",
    "first_query, first_texts = query_retrieved_pairs_dense[0]\n",
    "print(\"Query:\", first_query)\n",
    "print(\"Number of retrieved texts:\", len(first_texts))\n",
    "print(\"First retrieved text snippet:\", first_texts[0][:200].replace(\"\\n\", \" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c853d9",
   "metadata": {},
   "source": [
    "Same with the sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e33200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Number of retrieved texts: 2\n",
      "First retrieved text snippet: It is possible to override all data.  Adjust  the slope of the decay, if needed, or edit data to test theories  about the effects of changes you can make to get specific results.   Data edited in this ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your BM25 output JSON\n",
    "path = r\"retrieval_results_sparse copy.json\"\n",
    "\n",
    "# Load the JSON\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bm25_data = json.load(f)\n",
    "\n",
    "# Build a list of tuples: (query, [list of retrieved texts])\n",
    "query_retrieved_pairs_sparse = [\n",
    "    (\n",
    "        item[\"query\"],\n",
    "        [res[\"window\"].strip() for res in item[\"results\"]]\n",
    "    )\n",
    "    for item in bm25_data\n",
    "]\n",
    "\n",
    "# Example: show the first query and its retrieved texts\n",
    "first_query, first_texts = query_retrieved_pairs_sparse[0]\n",
    "print(\"Query:\", first_query)\n",
    "print(\"Number of retrieved texts:\", len(first_texts))\n",
    "print(\"First retrieved text snippet:\", first_texts[0][:200].replace(\"\\n\", \" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9903e9a",
   "metadata": {},
   "source": [
    "LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa255af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense pairs found: 18\n",
      "Sparse pairs found: 18\n"
     ]
    }
   ],
   "source": [
    "path = r\"chat_logs.json\"\n",
    "\n",
    "# Load the JSON\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dense_pairs = []\n",
    "sparse_pairs = []\n",
    "\n",
    "for item in data:\n",
    "    query = item.get(\"query\", \"\").strip()\n",
    "    mode = item.get(\"mode\", \"\").strip().lower()  # normalize to lowercase\n",
    "    \n",
    "    if mode == \"dense rag\":\n",
    "        response = item.get(\"response\", \"\").strip()\n",
    "        dense_pairs.append((query, response))\n",
    "    elif mode == \"sparse rag\":\n",
    "        results = item.get(\"results\", [])\n",
    "        texts = [res.get(\"window\", \"\").strip() for res in results]  # even if empty\n",
    "        sparse_pairs.append((query, texts))\n",
    "\n",
    "print(\"Dense pairs found:\", len(dense_pairs))\n",
    "print(\"Sparse pairs found:\", len(sparse_pairs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceade81b",
   "metadata": {},
   "source": [
    "### Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d8670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense - Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution should I use if I want compatibility with analysis tools like matlab?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution is better for a university group? They want to use this solution in different applications.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need a solution that complies with noise regulations and does automated reports.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution should I use to measure noise levels in a factory floor?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: What solution should I use to see if a construction site follows the noise regulations?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution should I use to conduct noise impact assessments? I need GPS tagging and the device to be weatherproof.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I want to monitore a contruction site. What should I use? I need long term logging and report generation.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need a solution that covers, Long term logging, GPS tagging, weatherproofing and report generation.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solution should I use to measure a building design?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need a solution for covering Frequency analysis, reverberation time and standards compliance.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need to measure a residential building and I need to follow the standard ISO 3382. Which solution should I use?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which is the best solution to measure room acoustics?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need to perform sound power measurements with a multi channel setup. Which solution should I use?\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I need a solution that covers standard compliance to perform sound power measurements.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I want a solution with a simple interface and affordabe.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: I would like to have a solution for noise monitoring that is portable.\n",
      "Score: 0.0\n",
      "\n",
      "Dense - Query: Which solutions gives basic logging and a simple interface?\n",
      "Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure prerequisite variables are available\n",
    "if 'query_texts_pairs' not in globals() or 'query_retrieved_pairs_dense' not in globals():\n",
    "    raise NameError(\n",
    "        \"query_texts_pairs and/or query_retrieved_pairs_dense are not defined. \"\n",
    "        \"Please run the cells that load ground truth (cell that creates query_texts_pairs) \"\n",
    "        \"and dense retrieval results (cell that creates query_retrieved_pairs_dense) before this cell.\"\n",
    "    )\n",
    "\n",
    "# Initialize metric\n",
    "context_precision = NonLLMContextPrecisionWithReference()\n",
    "\n",
    "# Store scores\n",
    "dense_scores = []\n",
    "sparse_scores = []\n",
    "\n",
    "# Loop over all queries for Dense retrieval\n",
    "# Note: query_texts_pairs elements are (query, ground_truth_answer, [reference_texts])\n",
    "for (query_gt, gt_answer, gt_texts), (query_dense, dense_texts) in zip(query_texts_pairs, query_retrieved_pairs_dense):\n",
    "    # Sanity check: queries should match\n",
    "    assert query_gt == query_dense, f\"Query mismatch: {query_gt} vs {query_dense}\"\n",
    "\n",
    "    # Build sample (NonLLM metric expects reference_contexts + retrieved_contexts)\n",
    "    sample = SingleTurnSample(\n",
    "        retrieved_contexts=dense_texts,\n",
    "        reference_contexts=gt_texts\n",
    "    )\n",
    "\n",
    "    # Compute score (async)\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    dense_scores.append((query_gt, score))\n",
    "    print(f\"Dense - Query: {query_gt}\\nScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d7af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for query: Which solution should a researcher use if they want High accuracy and data export? is 0.0\n",
      "Score for query: Which solution should I use if I want compatibility with analysis tools like matlab? is 0.0\n",
      "Score for query: Which solution is better for a university group? They want to use this solution in different applications. is 0.0\n",
      "Score for query: I need a solution that complies with noise regulations and does automated reports. is 0.99999999995\n",
      "Score for query: Which solution should I use to measure noise levels in a factory floor? is 0.0\n",
      "Score for query: What solution should I use to see if a construction site follows the noise regulations? is 0.99999999995\n",
      "Score for query: Which solution should I use to conduct noise impact assessments? I need GPS tagging and the device to be weatherproof. is 0.0\n",
      "Score for query: I want to monitore a contruction site. What should I use? I need long term logging and report generation. is 0.0\n",
      "Score for query: I need a solution that covers, Long term logging, GPS tagging, weatherproofing and report generation. is 0.0\n",
      "Score for query: Which solution should I use to measure a building design? is 0.0\n",
      "Score for query: I need a solution for covering Frequency analysis, reverberation time and standards compliance. is 0.0\n",
      "Score for query: I need to measure a residential building and I need to follow the standard ISO 3382. Which solution should I use? is 0.0\n",
      "Score for query: Which is the best solution to measure room acoustics? is 0.0\n",
      "Score for query: I need to perform sound power measurements with a multi channel setup. Which solution should I use? is 0.0\n",
      "Score for query: I need a solution that covers standard compliance to perform sound power measurements. is 0.99999999995\n",
      "Score for query: I want a solution with a simple interface and affordabe. is 0.0\n",
      "Score for query: I would like to have a solution for noise monitoring that is portable. is 0.0\n",
      "Score for query: Which solutions gives basic logging and a simple interface? is 0.0\n"
     ]
    }
   ],
   "source": [
    "context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
    "\n",
    "dense_scores = []\n",
    "\n",
    "for (query_gt, gt_answer, gt_texts), (query_dense, dense_texts) in zip(query_texts_pairs, query_retrieved_pairs_dense):\n",
    "    # Sanity check: queries should match\n",
    "    assert query_gt == query_dense, f\"Query mismatch: {query_gt} vs {query_dense}\"\n",
    "\n",
    "    # Build sample\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        reference=gt_answer,  # This is the ground truth answer\n",
    "        retrieved_contexts=dense_texts\n",
    "    )\n",
    "\n",
    "    # Compute score\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    print(\"Score for query:\", query_gt, \"is\", score)\n",
    "    dense_scores.append((query_gt, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3896ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SingleTurnSample\nresponse\n  Input should be a valid string [type=string_type, input_value=[], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m query_gt == query_sparse, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery mismatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_gt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_sparse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Build sample\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m sample = \u001b[43mSingleTurnSample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43manswer_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This is the ground truth answer\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretrieved_contexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43msparse_texts\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Compute score\u001b[39;00m\n\u001b[32m     19\u001b[39m score = \u001b[38;5;28;01mawait\u001b[39;00m context_precision.single_turn_ascore(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nerea\\anaconda3\\envs\\thesis\\Lib\\site-packages\\pydantic\\main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for SingleTurnSample\nresponse\n  Input should be a valid string [type=string_type, input_value=[], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "\n",
    "context_precision = LLMContextPrecisionWithoutReference(llm=evaluator_llm)\n",
    "\n",
    "sparse_scores = []\n",
    "\n",
    "for (query_gt, gt_answer, gt_texts), (query_sparse, sparse_texts), (query_dense, answer_texts) in zip(query_texts_pairs, query_retrieved_pairs_sparse, sparse_pairs):\n",
    "    # Sanity check: queries should match\n",
    "    assert query_gt == query_sparse, f\"Query mismatch: {query_gt} vs {query_sparse}\"\n",
    "\n",
    "    # Build sample\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        response=answer_texts,  # This is the ground truth answer\n",
    "        retrieved_contexts=sparse_texts\n",
    "    )\n",
    "\n",
    "    # Compute score\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    print(\"Score for query:\", query_gt, \"is\", score)\n",
    "    sparse_scores.append((query_gt, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a7abf",
   "metadata": {},
   "source": [
    "Sparse one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b95e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse - Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution should I use if I want compatibility with analysis tools like matlab?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution is better for a university group? They want to use this solution in different applications.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need a solution that complies with noise regulations and does automated reports.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution should I use to measure noise levels in a factory floor?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: What solution should I use to see if a construction site follows the noise regulations?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution should I use to conduct noise impact assessments? I need GPS tagging and the device to be weatherproof.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I want to monitore a contruction site. What should I use? I need long term logging and report generation.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need a solution that covers, Long term logging, GPS tagging, weatherproofing and report generation.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solution should I use to measure a building design?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need a solution for covering Frequency analysis, reverberation time and standards compliance.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need to measure a residential building and I need to follow the standard ISO 3382. Which solution should I use?\n",
      "Score: 0.99999999995\n",
      "\n",
      "Sparse - Query: Which is the best solution to measure room acoustics?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need to perform sound power measurements with a multi channel setup. Which solution should I use?\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I need a solution that covers standard compliance to perform sound power measurements.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: I want a solution with a simple interface and affordabe.\n",
      "Score: 0.9999999999\n",
      "\n",
      "Sparse - Query: I would like to have a solution for noise monitoring that is portable.\n",
      "Score: 0.0\n",
      "\n",
      "Sparse - Query: Which solutions gives basic logging and a simple interface?\n",
      "Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure prerequisite variables are available\n",
    "if 'query_texts_pairs' not in globals() or 'query_retrieved_pairs_dense' not in globals():\n",
    "    raise NameError(\n",
    "        \"query_texts_pairs and/or query_retrieved_pairs_dense are not defined. \"\n",
    "        \"Please run the cells that load ground truth (cell that creates query_texts_pairs) \"\n",
    "        \"and dense retrieval results (cell that creates query_retrieved_pairs_dense) before this cell.\"\n",
    "    )\n",
    "\n",
    "# Initialize metric\n",
    "context_precision = NonLLMContextPrecisionWithReference()\n",
    "\n",
    "# Store scores\n",
    "sparse_scores = []\n",
    "\n",
    "# Loop over all queries for Sparse retrieval\n",
    "# Note: query_texts_pairs elements are (query, ground_truth_answer, [reference_texts])\n",
    "for (query_gt, gt_answer, gt_texts), (query_sparse, sparse_texts) in zip(query_texts_pairs, query_retrieved_pairs_sparse):\n",
    "    # Sanity check: queries should match\n",
    "    assert query_gt == query_sparse, f\"Query mismatch: {query_gt} vs {query_sparse}\"\n",
    "\n",
    "    # Build sample (NonLLM metric expects reference_contexts + retrieved_contexts)\n",
    "    sample = SingleTurnSample(\n",
    "        retrieved_contexts=sparse_texts,\n",
    "        reference_contexts=gt_texts\n",
    "    )\n",
    "\n",
    "    # Compute score (async)\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    sparse_scores.append((query_gt, score))\n",
    "    print(f\"Sparse - Query: {query_gt}\\nScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7614ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for query: Which solution should a researcher use if they want High accuracy and data export? is 0.0\n",
      "Score for query: Which solution should I use if I want compatibility with analysis tools like matlab? is 0.0\n",
      "Score for query: Which solution is better for a university group? They want to use this solution in different applications. is 0.0\n",
      "Score for query: I need a solution that complies with noise regulations and does automated reports. is 0.9999999999\n",
      "Score for query: Which solution should I use to measure noise levels in a factory floor? is 0.0\n",
      "Score for query: What solution should I use to see if a construction site follows the noise regulations? is 0.49999999995\n",
      "Score for query: Which solution should I use to conduct noise impact assessments? I need GPS tagging and the device to be weatherproof. is 0.9999999999\n",
      "Score for query: I want to monitore a contruction site. What should I use? I need long term logging and report generation. is 0.0\n",
      "Score for query: I need a solution that covers, Long term logging, GPS tagging, weatherproofing and report generation. is 0.0\n",
      "Score for query: Which solution should I use to measure a building design? is 0.0\n",
      "Score for query: I need a solution for covering Frequency analysis, reverberation time and standards compliance. is 0.0\n",
      "Score for query: I need to measure a residential building and I need to follow the standard ISO 3382. Which solution should I use? is 0.0\n",
      "Score for query: Which is the best solution to measure room acoustics? is 0.0\n",
      "Score for query: I need to perform sound power measurements with a multi channel setup. Which solution should I use? is 0.0\n",
      "Score for query: I need a solution that covers standard compliance to perform sound power measurements. is 0.99999999995\n",
      "Score for query: I want a solution with a simple interface and affordabe. is 0.99999999995\n",
      "Score for query: I would like to have a solution for noise monitoring that is portable. is 0.0\n",
      "Score for query: Which solutions gives basic logging and a simple interface? is 0.0\n"
     ]
    }
   ],
   "source": [
    "context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
    "\n",
    "sparse_scores = []\n",
    "\n",
    "for (query_gt, gt_answer, gt_texts), (query_sparse, sparse_texts) in zip(query_texts_pairs, query_retrieved_pairs_sparse):\n",
    "    # Sanity check: queries should match\n",
    "    assert query_gt == query_sparse, f\"Query mismatch: {query_gt} vs {query_sparse}\"\n",
    "\n",
    "    # Build sample\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        reference=gt_answer,  # This is the ground truth answer\n",
    "        retrieved_contexts=sparse_texts\n",
    "    )\n",
    "\n",
    "    # Compute score\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    print(\"Score for query:\", query_gt, \"is\", score)\n",
    "    sparse_scores.append((query_gt, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad8bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for query: Which solution should a researcher use if they want High accuracy and data export? is 0.0\n",
      "Score for query: Which solution should I use if I want compatibility with analysis tools like matlab? is 0.0\n",
      "Score for query: Which solution is better for a university group? They want to use this solution in different applications. is 0.0\n",
      "Score for query: I need a solution that complies with noise regulations and does automated reports. is 0.99999999995\n",
      "Score for query: Which solution should I use to measure noise levels in a factory floor? is 0.99999999995\n",
      "Score for query: What solution should I use to see if a construction site follows the noise regulations? is 0.49999999995\n",
      "Score for query: Which solution should I use to conduct noise impact assessments? I need GPS tagging and the device to be weatherproof. is 0.99999999995\n",
      "Score for query: I want to monitore a contruction site. What should I use? I need long term logging and report generation. is 0.9999999999\n",
      "Score for query: I need a solution that covers, Long term logging, GPS tagging, weatherproofing and report generation. is 0.99999999995\n",
      "Score for query: Which solution should I use to measure a building design? is 0.0\n",
      "Score for query: I need a solution for covering Frequency analysis, reverberation time and standards compliance. is 0.49999999995\n",
      "Score for query: I need to measure a residential building and I need to follow the standard ISO 3382. Which solution should I use? is 0.99999999995\n",
      "Score for query: Which is the best solution to measure room acoustics? is 0.0\n",
      "Score for query: I need to perform sound power measurements with a multi channel setup. Which solution should I use? is 0.99999999995\n",
      "Score for query: I need a solution that covers standard compliance to perform sound power measurements. is 0.99999999995\n",
      "Score for query: I want a solution with a simple interface and affordabe. is 0.99999999995\n",
      "Score for query: I would like to have a solution for noise monitoring that is portable. is 0.0\n",
      "Score for query: Which solutions gives basic logging and a simple interface? is 0.0\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "\n",
    "context_precision = LLMContextPrecisionWithoutReference(llm=evaluator_llm)\n",
    "\n",
    "sparse_scores = []\n",
    "\n",
    "for (query_gt, gt_answer, gt_texts), (query_sparse, sparse_texts), (query_dense, answer_texts) in zip(query_texts_pairs, query_retrieved_pairs_sparse, dense_pairs):\n",
    "    # Sanity check: queries should match\n",
    "    assert query_gt == query_sparse, f\"Query mismatch: {query_gt} vs {query_sparse}\"\n",
    "\n",
    "    # Build sample\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        response=answer_texts,  # This is the ground truth answer\n",
    "        retrieved_contexts=sparse_texts\n",
    "    )\n",
    "\n",
    "    # Compute score\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    print(\"Score for query:\", query_gt, \"is\", score)\n",
    "    sparse_scores.append((query_gt, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
