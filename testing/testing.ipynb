{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24481ef7",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082641f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NCABALLERO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import NonLLMContextPrecisionWithReference\n",
    "from ragas.metrics import LLMContextPrecisionWithReference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addb58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM  # Updated import\n",
    "   \n",
    "evaluator_llm = OllamaLLM(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a54ae",
   "metadata": {},
   "source": [
    "Read Ground truth and get var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199e1224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Number of ground truth texts: 17\n",
      "First text snippet: You can simultaneously measure source (L1) and receiving room levels (L2) by connecting two HBK 2255 Sound Level Meters to the app.  This feature is designed to save time and enhance efficiency, espec ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:/Users/NCABALLERO/OneDrive - HBK/Thesis/Code/masterThesis/testing/gt.json\"\n",
    "\n",
    "# Load the JSON safely\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build a list of tuples: (query, [list of texts])\n",
    "query_texts_pairs = [\n",
    "    (\n",
    "        item[\"question\"].strip(),\n",
    "        [seg[\"text\"].strip() for seg in item.get(\"ground_truth_segments\", [])]\n",
    "    )\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Example: show the first query and its texts\n",
    "first_query, first_texts = query_texts_pairs[0]\n",
    "print(\"Query:\", first_query)\n",
    "print(\"Number of ground truth texts:\", len(first_texts))\n",
    "print(\"First text snippet:\", first_texts[0][:200].replace(\"\\n\", \" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ad22d",
   "metadata": {},
   "source": [
    "read dense json and get question and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea936817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Number of retrieved texts: 2\n",
      "First retrieved text snippet: Pros and cons of measurement methods Scans can be faster than measuring at fixed pos- You are able to listen to the sound field as you You can control measurements from outside the room, without intro ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your RAG output JSON\n",
    "path = r\"C:/Users/NCABALLERO/OneDrive - HBK/Thesis/Code/masterThesis/testing/retrieval_results.json\"\n",
    "\n",
    "# Load the JSON\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    rag_data_dense = json.load(f)\n",
    "\n",
    "# Build a list of tuples: (query, [list of retrieved texts])\n",
    "query_retrieved_pairs_dense = [\n",
    "    (\n",
    "        query,\n",
    "        [item[\"text\"].strip() for item in texts]\n",
    "    )\n",
    "    for query, texts in rag_data_dense.items()\n",
    "]\n",
    "\n",
    "# Example: show the first query and its retrieved texts\n",
    "first_query, first_texts = query_retrieved_pairs_dense[0]\n",
    "print(\"Query:\", first_query)\n",
    "print(\"Number of retrieved texts:\", len(first_texts))\n",
    "print(\"First retrieved text snippet:\", first_texts[0][:200].replace(\"\\n\", \" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c853d9",
   "metadata": {},
   "source": [
    "Same with the sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e33200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which solution should a researcher use if they want High accuracy and data export?\n",
      "Number of retrieved texts: 2\n",
      "First retrieved text snippet: It is possible to override all data.  Adjust  the slope of the decay, if needed, or edit data to test theories  about the effects of changes you can make to get specific results.   Data edited in this ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your BM25 output JSON\n",
    "path = r\"C:/Users/NCABALLERO/OneDrive - HBK/Thesis/Code/masterThesis/testing/retrieval_results_sparse copy.json\"\n",
    "\n",
    "# Load the JSON\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bm25_data = json.load(f)\n",
    "\n",
    "# Build a list of tuples: (query, [list of retrieved texts])\n",
    "query_retrieved_pairs_sparse = [\n",
    "    (\n",
    "        item[\"query\"],\n",
    "        [res[\"window\"].strip() for res in item[\"results\"]]\n",
    "    )\n",
    "    for item in bm25_data\n",
    "]\n",
    "\n",
    "# Example: show the first query and its retrieved texts\n",
    "first_query, first_texts = query_retrieved_pairs_sparse[0]\n",
    "print(\"Query:\", first_query)\n",
    "print(\"Number of retrieved texts:\", len(first_texts))\n",
    "print(\"First retrieved text snippet:\", first_texts[0][:200].replace(\"\\n\", \" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9903e9a",
   "metadata": {},
   "source": [
    "LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa255af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense pairs found: 18\n",
      "Sparse pairs found: 18\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:/Users/NCABALLERO/OneDrive - HBK/Thesis/Code/masterThesis/testing/chat_logs.json\"\n",
    "\n",
    "# Load the JSON\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dense_pairs = []\n",
    "sparse_pairs = []\n",
    "\n",
    "for item in data:\n",
    "    query = item.get(\"query\", \"\").strip()\n",
    "    mode = item.get(\"mode\", \"\").strip().lower()  # normalize to lowercase\n",
    "    \n",
    "    if mode == \"dense rag\":\n",
    "        response = item.get(\"response\", \"\").strip()\n",
    "        dense_pairs.append((query, response))\n",
    "    elif mode == \"sparse rag\":\n",
    "        results = item.get(\"results\", [])\n",
    "        texts = [res.get(\"window\", \"\").strip() for res in results]  # even if empty\n",
    "        sparse_pairs.append((query, texts))\n",
    "\n",
    "print(\"Dense pairs found:\", len(dense_pairs))\n",
    "print(\"Sparse pairs found:\", len(sparse_pairs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceade81b",
   "metadata": {},
   "source": [
    "### Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d8670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense RAG scores:\n",
      "Which solution should a researcher use if they want High accuracy and data export? -> 0.0\n",
      "Which solution should I use if I want compatibility with analysis tools like matlab? -> 0.0\n",
      "Which solution is better for a university group? They want to use this solution in different applications. -> 0.0\n",
      "I need a solution that complies with noise regulations and does automated reports. -> 0.0\n",
      "Which solution should I use to measure noise levels in a factory floor? -> 0.0\n",
      "What solution should I use to see if a construction site follows the noise regulations? -> 0.0\n",
      "Which solution should I use to conduct noise impact assessments? I need GPS tagging and the device to be weatherproof. -> 0.0\n",
      "I want to monitore a contruction site. What should I use? I need long term logging and report generation. -> 0.0\n",
      "I need a solution that covers, Long term logging, GPS tagging, weatherproofing and report generation. -> 0.0\n",
      "Which solution should I use to measure a building design? -> 0.0\n",
      "I need a solution for covering Frequency analysis, reverberation time and standards compliance. -> 0.0\n",
      "I need to measure a residential building and I need to follow the standard ISO 3382. Which solution should I use? -> 0.0\n",
      "Which is the best solution to measure room acoustics? -> 0.0\n",
      "I need to perform sound power measurements with a multi channel setup. Which solution should I use? -> 0.0\n",
      "I need a solution that covers standard compliance to perform sound power measurements. -> 0.0\n",
      "I want a solution with a simple interface and affordabe. -> 0.0\n",
      "I would like to have a solution for noise monitoring that is portable. -> 0.0\n",
      "Which solutions gives basic logging and a simple interface? -> 0.0\n",
      "\n",
      "Sparse RAG scores:\n",
      "Which solution should a researcher use if they want High accuracy and data export? -> 0.0\n",
      "Which solution should I use if I want compatibility with analysis tools like matlab? -> 0.0\n",
      "Which solution is better for a university group? They want to use this solution in different applications. -> 0.0\n",
      "I need a solution that complies with noise regulations and does automated reports. -> 0.0\n",
      "Which solution should I use to measure noise levels in a factory floor? -> 0.0\n",
      "What solution should I use to see if a construction site follows the noise regulations? -> 0.0\n",
      "Which solution should I use to conduct noise impact assessments? I need GPS tagging and the device to be weatherproof. -> 0.0\n",
      "I want to monitore a contruction site. What should I use? I need long term logging and report generation. -> 0.0\n",
      "I need a solution that covers, Long term logging, GPS tagging, weatherproofing and report generation. -> 0.0\n",
      "Which solution should I use to measure a building design? -> 0.0\n",
      "I need a solution for covering Frequency analysis, reverberation time and standards compliance. -> 0.0\n",
      "I need to measure a residential building and I need to follow the standard ISO 3382. Which solution should I use? -> 0.99999999995\n",
      "Which is the best solution to measure room acoustics? -> 0.0\n",
      "I need to perform sound power measurements with a multi channel setup. Which solution should I use? -> 0.0\n",
      "I need a solution that covers standard compliance to perform sound power measurements. -> 0.0\n",
      "I want a solution with a simple interface and affordabe. -> 0.9999999999\n",
      "I would like to have a solution for noise monitoring that is portable. -> 0.0\n",
      "Which solutions gives basic logging and a simple interface? -> 0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize metric\n",
    "context_precision = NonLLMContextPrecisionWithReference()\n",
    "\n",
    "# Store scores\n",
    "dense_scores = []\n",
    "sparse_scores = []\n",
    "\n",
    "# Loop over all queries for Dense retrieval\n",
    "for (query_gt, gt_texts), (query_dense, dense_texts) in zip(query_texts_pairs, query_retrieved_pairs_dense):\n",
    "    # Sanity check: queries should match\n",
    "    assert query_gt == query_dense, f\"Query mismatch: {query_gt} vs {query_dense}\"\n",
    "\n",
    "    # Build sample\n",
    "    sample = SingleTurnSample(\n",
    "        retrieved_contexts=dense_texts, \n",
    "        reference_contexts=gt_texts\n",
    "    )\n",
    "\n",
    "    # Compute score\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    dense_scores.append((query_gt, score))\n",
    "\n",
    "# Loop over all queries for Sparse retrieval\n",
    "for (query_gt, gt_texts), (query_sparse, sparse_texts) in zip(query_texts_pairs, query_retrieved_pairs_sparse):\n",
    "    assert query_gt == query_sparse, f\"Query mismatch: {query_gt} vs {query_sparse}\"\n",
    "\n",
    "    sample = SingleTurnSample(\n",
    "        retrieved_contexts=sparse_texts, \n",
    "        reference_contexts=gt_texts\n",
    "    )\n",
    "\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    sparse_scores.append((query_gt, score))\n",
    "\n",
    "# Example: show results\n",
    "print(\"Dense RAG scores:\")\n",
    "for q, s in dense_scores:\n",
    "    print(q, \"->\", s)\n",
    "\n",
    "print(\"\\nSparse RAG scores:\")\n",
    "for q, s in sparse_scores:\n",
    "    print(q, \"->\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7af2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Verification\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='This output is a JSON (J... and measurement modes.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     14\u001b[39m     dense_llm_answer = \u001b[38;5;28mnext\u001b[39m((resp \u001b[38;5;28;01mfor\u001b[39;00m q, resp \u001b[38;5;129;01min\u001b[39;00m dense_pairs \u001b[38;5;28;01mif\u001b[39;00m q == query_gt), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     16\u001b[39m     sample = SingleTurnSample(\n\u001b[32m     17\u001b[39m         user_input=query_gt,\n\u001b[32m     18\u001b[39m         reference=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(gt_texts),          \u001b[38;5;66;03m# combine all ground truth texts\u001b[39;00m\n\u001b[32m     19\u001b[39m         retrieved_contexts=dense_texts + ([dense_llm_answer] \u001b[38;5;28;01mif\u001b[39;00m dense_llm_answer \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[32m     20\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     score = \u001b[38;5;28;01mawait\u001b[39;00m context_precision.single_turn_ascore(sample)\n\u001b[32m     23\u001b[39m     dense_llm_scores.append((query_gt, score))\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# --- Sparse retrieval with LLM answer ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NCABALLERO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ragas\\metrics\\base.py:561\u001b[39m, in \u001b[36mSingleTurnMetric.single_turn_ascore\u001b[39m\u001b[34m(self, sample, callbacks, timeout)\u001b[39m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm.ended:\n\u001b[32m    560\u001b[39m         rm.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm.ended:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NCABALLERO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ragas\\metrics\\base.py:554\u001b[39m, in \u001b[36mSingleTurnMetric.single_turn_ascore\u001b[39m\u001b[34m(self, sample, callbacks, timeout)\u001b[39m\n\u001b[32m    547\u001b[39m rm, group_cm = new_group(\n\u001b[32m    548\u001b[39m     \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    549\u001b[39m     inputs=sample.to_dict(),\n\u001b[32m    550\u001b[39m     callbacks=callbacks,\n\u001b[32m    551\u001b[39m     metadata={\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: ChainType.METRIC},\n\u001b[32m    552\u001b[39m )\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     score = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait_for(\n\u001b[32m    555\u001b[39m         \u001b[38;5;28mself\u001b[39m._single_turn_ascore(sample=sample, callbacks=group_cm),\n\u001b[32m    556\u001b[39m         timeout=timeout,\n\u001b[32m    557\u001b[39m     )\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm.ended:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NCABALLERO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:507\u001b[39m, in \u001b[36mwait_for\u001b[39m\u001b[34m(fut, timeout)\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m timeouts.timeout(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NCABALLERO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138\u001b[39m, in \u001b[36mLLMContextPrecisionWithReference._single_turn_ascore\u001b[39m\u001b[34m(self, sample, callbacks)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_single_turn_ascore\u001b[39m(\n\u001b[32m    135\u001b[39m     \u001b[38;5;28mself\u001b[39m, sample: SingleTurnSample, callbacks: Callbacks\n\u001b[32m    136\u001b[39m ) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m    137\u001b[39m     row = sample.to_dict()\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ascore(row, callbacks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NCABALLERO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:152\u001b[39m, in \u001b[36mLLMContextPrecisionWithReference._ascore\u001b[39m\u001b[34m(self, row, callbacks)\u001b[39m\n\u001b[32m    148\u001b[39m responses = []\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m context \u001b[38;5;129;01min\u001b[39;00m retrieved_contexts:\n\u001b[32m    150\u001b[39m     verdicts: t.List[\n\u001b[32m    151\u001b[39m         Verification\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     ] = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.context_precision_prompt.generate_multiple(\n\u001b[32m    153\u001b[39m         data=QAC(\n\u001b[32m    154\u001b[39m             question=user_input,\n\u001b[32m    155\u001b[39m             context=context,\n\u001b[32m    156\u001b[39m             answer=reference,\n\u001b[32m    157\u001b[39m         ),\n\u001b[32m    158\u001b[39m         llm=\u001b[38;5;28mself\u001b[39m.llm,\n\u001b[32m    159\u001b[39m         callbacks=callbacks,\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     responses.append([result.model_dump() \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m verdicts])\n\u001b[32m    164\u001b[39m answers = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NCABALLERO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:288\u001b[39m, in \u001b[36mPydanticPrompt.generate_multiple\u001b[39m\u001b[34m(self, llm, data, n, temperature, stop, callbacks, retries_left)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    285\u001b[39m     \u001b[38;5;66;03m# For the parser, we need a BaseRagasLLM, so if it's a LangChain LLM, we need to handle this\u001b[39;00m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_langchain_llm(llm):\n\u001b[32m    287\u001b[39m         \u001b[38;5;66;03m# Skip parsing retry for LangChain LLMs since parser expects BaseRagasLLM\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m         answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m         ragas_llm = t.cast(BaseRagasLLM, llm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NCABALLERO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\main.py:766\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, extra, context, by_alias, by_name)\u001b[39m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    761\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    762\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    763\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    764\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Verification\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='This output is a JSON (J... and measurement modes.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
     ]
    }
   ],
   "source": [
    "# Assume you already have an LLM evaluator instance\n",
    "context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
    "\n",
    "dense_llm_scores = []\n",
    "sparse_llm_scores = []\n",
    "\n",
    "# --- Dense retrieval with LLM answer ---\n",
    "for (query_gt, gt_texts), (query_dense, dense_texts) in zip(query_texts_pairs, query_retrieved_pairs_dense):\n",
    "    # Sanity check\n",
    "    if query_gt != query_dense:\n",
    "        print(f\"Warning: query mismatch: {query_gt} vs {query_dense}\")\n",
    "\n",
    "    # Wrap the LLM answer if you also want to score it\n",
    "    dense_llm_answer = next((resp for q, resp in dense_pairs if q == query_gt), None)\n",
    "\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        reference=\" \".join(gt_texts),          # combine all ground truth texts\n",
    "        retrieved_contexts=dense_texts + ([dense_llm_answer] if dense_llm_answer else [])\n",
    "    )\n",
    "\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    dense_llm_scores.append((query_gt, score))\n",
    "\n",
    "# --- Sparse retrieval with LLM answer ---\n",
    "for (query_gt, gt_texts), (query_sparse, sparse_texts) in zip(query_texts_pairs, query_retrieved_pairs_sparse):\n",
    "    if query_gt != query_sparse:\n",
    "        print(f\"Warning: query mismatch: {query_gt} vs {query_sparse}\")\n",
    "\n",
    "    sparse_llm_answer = next((resp for q, resp in sparse_pairs if q == query_gt), None)\n",
    "\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query_gt,\n",
    "        reference=\" \".join(gt_texts),\n",
    "        retrieved_contexts=sparse_texts + ([sparse_llm_answer] if sparse_llm_answer else [])\n",
    "    )\n",
    "\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    sparse_llm_scores.append((query_gt, score))\n",
    "\n",
    "# --- Example outputs ---\n",
    "print(\"Dense LLM scores (first 3):\")\n",
    "for q, s in dense_llm_scores:\n",
    "    print(q, \"->\", s)\n",
    "\n",
    "print(\"\\nSparse LLM scores (first 3):\")\n",
    "for q, s in sparse_llm_scores:\n",
    "    print(q, \"->\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95624f9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
