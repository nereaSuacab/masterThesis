{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc958b23",
   "metadata": {},
   "source": [
    "**Imports**\n",
    "\n",
    "* ***init_chat_model***: initializes a chat-based LLM\n",
    "* ***initialize_agent***: creates an agent, system that uses an LLM plus tools to perform reasining and actions\n",
    "* ***AgentType***: Enum to specify the type of agent\n",
    "* ***Tool***: Represents a tool (an action the agent can take), such as a web search, calculator, or database query\n",
    "* ***TavilySearchResults***: A community-contributed tool for querying Tavily Search API (web search for LLMs).\n",
    "* ***ConversationBufferMemory***: Memory that stores the entire conversation history\n",
    "* ***HumanMessage***: A message from the user\n",
    "* ***AIMessage***: A message from the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b92cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895c7ba",
   "metadata": {},
   "source": [
    "I set the LangSmith API (authenticate with LangSmith for tracing and analytics) key and the Tavily API key (perform web searches)\n",
    "\n",
    "- With these we can run LangChain agents with tools\n",
    "- Log and monitor runs in LangSmith\n",
    "- Use the Tavily Search tool for web lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6348fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\" # Enable LangSmith tracing\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"LANGSMITH_API_KEY: \\n\")\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY: \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355a210",
   "metadata": {},
   "source": [
    "We create a model using Llama3 model, provided by Ollama (downloaded in my computer).\n",
    "\n",
    "The settings for the model is 150 tokens and control randomness with temperature, set to 0, what means deterministic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ff0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model setup\n",
    "model = init_chat_model(\n",
    "    \"llama3\",\n",
    "    model_provider=\"ollama\",\n",
    "    model_kwargs={\"max_tokens\": 150, \"temperature\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe0ae5",
   "metadata": {},
   "source": [
    "I create a memeory module for tracking the chat history.\n",
    "\n",
    "When True, memory returns the conversation as a list of structured message objects. Preserves role information-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e41102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nerea\\AppData\\Local\\Temp\\ipykernel_11552\\722834312.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# Simple memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb519961",
   "metadata": {},
   "source": [
    "We create a minimal LangChain agent that uses the model, keeps track of conversation history and manually builds the propmpt.\n",
    "\n",
    "* **CHAT function** -> Gets the conversation history, create a base system prompt, append the last 6 messages, add the new user message, calls the model, saves the turn to memory and return the response.\n",
    "* **SHOW_MEMORY fuction** -> Prints all stored messages in the conversation memory for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961e1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, model, memory):\n",
    "        self.model = model\n",
    "        self.memory = memory\n",
    "    \n",
    "    def chat(self, message):\n",
    "        # Get conversation history\n",
    "        history = self.memory.chat_memory.messages\n",
    "        \n",
    "        # Create context with history\n",
    "        context = \"You are a helpful assistant. Remember information from our conversation.\\n\\n\"\n",
    "        \n",
    "        # Add conversation history\n",
    "        for msg in history[-6:]:  # Last 6 messages only\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                context += f\"Human: {msg.content}\\n\"\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                context += f\"Assistant: {msg.content}\\n\"\n",
    "        \n",
    "        # Add current message\n",
    "        context += f\"Human: {message}\\nAssistant:\"\n",
    "        \n",
    "        try:\n",
    "            # Get response\n",
    "            response = self.model.invoke(context)\n",
    "            response_text = response.content\n",
    "            \n",
    "            # Save to memory\n",
    "            self.memory.chat_memory.add_user_message(message)\n",
    "            self.memory.chat_memory.add_ai_message(response_text)\n",
    "            \n",
    "            return response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def show_memory(self):\n",
    "        messages = self.memory.chat_memory.messages\n",
    "        print(f\"Memory has {len(messages)} messages:\")\n",
    "        for i, msg in enumerate(messages):\n",
    "            msg_type = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "            print(f\"{i}: {msg_type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db15de42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Simple Agent ===\n",
      "\n",
      "1. Greeting with name...\n",
      "Agent: Nice to meet you, Nerea! I'll make sure to remember your name throughout our conversation. Got it!\n"
     ]
    }
   ],
   "source": [
    "# Create and test the simple agent\n",
    "print(\"=== Creating Simple Agent ===\")\n",
    "simple_agent = SimpleAgent(model, memory)\n",
    "\n",
    "# Test 1: Greeting with name\n",
    "print(\"\\n1. Greeting with name...\")\n",
    "response1 = simple_agent.chat(\"Hello! My name is Nerea. Please remember this for our conversation.\")\n",
    "print(f\"Agent: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b5203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Testing memory...\n",
      "Agent: Your name is Nerea!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Testing memory...\")\n",
    "response2 = simple_agent.chat(\"What is my name?\")\n",
    "print(f\"Agent: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b4b5995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Memory Contents ===\n",
      "Memory has 4 messages:\n",
      "0: Human: Hello! My name is Nerea. Please remember this for our conversation.\n",
      "1: AI: Nice to meet you, Nerea! I'll make sure to remember your name throughout our conversation. Got it!\n",
      "2: Human: What is my name?\n",
      "3: AI: Your name is Nerea!\n"
     ]
    }
   ],
   "source": [
    "# Show memory contents\n",
    "print(\"\\n=== Memory Contents ===\")\n",
    "simple_agent.show_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef525a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Simple conversation...\n",
      "Agent: You live in Denmark! I've got that note down!\n",
      "Agent: Your name is Nerea!\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Simple conversation\n",
    "print(\"\\n3. Simple conversation...\")\n",
    "response3 = simple_agent.chat(\"I live in denmark.\")\n",
    "response4 = simple_agent.chat(\"What was my name again?\")\n",
    "print(f\"Agent: {response3}\")\n",
    "print(f\"Agent: {response4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "160b8558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Let me see... Based on our conversation, here's what I know about you:\n",
      "\n",
      "You're named Nerea! And you live in Denmark!\n",
      "\n",
      "That's all I've got so far!\n"
     ]
    }
   ],
   "source": [
    "response5 = simple_agent.chat(\"Tell me a short description about me with the things I have told you.\")\n",
    "print(f\"Agent: {response5}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
